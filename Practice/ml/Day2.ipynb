{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42251f70-847f-42d4-882d-4894d988721c",
   "metadata": {},
   "source": [
    "* STEP 1 â€” Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea88dfb0-7796-40f5-a3ec-76508ee53484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>loan_amount</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>30000</td>\n",
       "      <td>200000</td>\n",
       "      <td>650</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>120000</td>\n",
       "      <td>500000</td>\n",
       "      <td>720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>60000</td>\n",
       "      <td>300000</td>\n",
       "      <td>680</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>150000</td>\n",
       "      <td>800000</td>\n",
       "      <td>750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>25000</td>\n",
       "      <td>150000</td>\n",
       "      <td>640</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  income  loan_amount  credit_score  default\n",
       "0   25   30000       200000           650        0\n",
       "1   45  120000       500000           720        0\n",
       "2   35   60000       300000           680        0\n",
       "3   50  150000       800000           750        0\n",
       "4   23   25000       150000           640        1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"age\": [25,45,35,50,23,40,29,55,31,48,26,37],\n",
    "    \"income\": [30000,120000,60000,150000,25000,90000,35000,200000,45000,130000,28000,75000],\n",
    "    \"loan_amount\": [200000,500000,300000,800000,150000,400000,250000,900000,270000,600000,180000,350000],\n",
    "    \"credit_score\": [650,720,680,750,640,700,660,780,670,740,645,690],\n",
    "    \"default\": [0,0,0,0,1,0,1,0,1,0,1,0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fd44db-56f8-4bfd-8fbc-bccd4cb40083",
   "metadata": {},
   "source": [
    "* STEP 2 â€” Feature / Target Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbaf8f50-e4e7-4c37-9221-6001417a42cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"default\", axis=1)\n",
    "y = df[\"default\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745abc5c-fc31-40e6-a6b2-e43502b8cc05",
   "metadata": {},
   "source": [
    "* STEP 3 â€” Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89b1ea4d-f033-4f6c-94e3-804a02f3d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c1650-8a9a-4bf3-b425-3c13406cbe4f",
   "metadata": {},
   "source": [
    "* STEP 4 â€” kNN WITHOUT Scaling (Wrong way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "213f5345-2504-4bab-8a49-a74e11541e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Scaling - Train AUC: 0.9\n",
      "No Scaling - Test  AUC: 1.0\n"
     ]
    }
   ],
   "source": [
    "knn_no_scaling = KNeighborsClassifier(\n",
    "    n_neighbors=5,\n",
    "    metric=\"euclidean\"\n",
    ")\n",
    "\n",
    "knn_no_scaling.fit(X_train, y_train)\n",
    "\n",
    "train_auc_ns = roc_auc_score(\n",
    "    y_train,\n",
    "    knn_no_scaling.predict_proba(X_train)[:,1]\n",
    ")\n",
    "\n",
    "test_auc_ns = roc_auc_score(\n",
    "    y_test,\n",
    "    knn_no_scaling.predict_proba(X_test)[:,1]\n",
    ")\n",
    "\n",
    "print(\"No Scaling - Train AUC:\", train_auc_ns)\n",
    "print(\"No Scaling - Test  AUC:\", test_auc_ns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36c38e7-b051-4172-8f4c-29ac4aba5e19",
   "metadata": {},
   "source": [
    "`What this shows`\n",
    "\n",
    "Income & loan_amount dominate distance\n",
    "\n",
    "Age & credit_score almost ignored\n",
    "\n",
    "Distance bias present"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baed13b-9def-44cf-9ad4-ffc71a3984f0",
   "metadata": {},
   "source": [
    "* STEP 5: kNN WITH Scaling(Correct way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d39b7997-e985-4e6a-bae3-48a38bc76ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITH Scaling\n",
      "Train AUC: 0.9\n",
      "Test  AUC: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
      "        capture_output=True,\n",
      "        text=True,\n",
      "    )\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\subprocess.py\", line 554, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\subprocess.py\", line 1039, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        pass_fds, cwd, env,\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "                        gid, gids, uid, umask,\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        start_new_session, process_group)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\subprocess.py\", line 1554, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "                             # no special security\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "                             cwd,\n",
      "                             ^^^^\n",
      "                             startupinfo)\n",
      "                             ^^^^^^^^^^^^\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "knn_scaled = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"knn\", KNeighborsClassifier(n_neighbors=5))\n",
    "])\n",
    "\n",
    "knn_scaled.fit(X_train, y_train)\n",
    "\n",
    "train_auc_s = roc_auc_score(\n",
    "    y_train, knn_scaled.predict_proba(X_train)[:, 1]\n",
    ")\n",
    "test_auc_s = roc_auc_score(\n",
    "    y_test, knn_scaled.predict_proba(X_test)[:, 1]\n",
    ")\n",
    "\n",
    "print(\"\\nWITH Scaling\")\n",
    "print(\"Train AUC:\", train_auc_s)\n",
    "print(\"Test  AUC:\", test_auc_s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913f0034-ffb2-4c77-af3d-93ad6712bf02",
   "metadata": {},
   "source": [
    "`What this shows`\n",
    "\n",
    "All features contribute fairly\n",
    "\n",
    "Distance becomes meaningful\n",
    "\n",
    "Generalization improves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48a92c4-d310-45c3-8a79-35cd98db2c53",
   "metadata": {},
   "source": [
    "* STEP 6: Biasâ€“Variance via k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac0b0af8-00ae-4686-889b-0b2312edbd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1 | Train AUC=1.000 | Test AUC=1.000\n",
      "k=3 | Train AUC=0.900 | Test AUC=1.000\n",
      "k=5 | Train AUC=0.900 | Test AUC=1.000\n",
      "k=7 | Train AUC=0.700 | Test AUC=0.833\n"
     ]
    }
   ],
   "source": [
    "k_values = [1, 3, 5, 7]\n",
    "\n",
    "for k in k_values:\n",
    "    model = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"knn\", KNeighborsClassifier(n_neighbors=k))\n",
    "    ])\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_auc = roc_auc_score(\n",
    "        y_train, model.predict_proba(X_train)[:, 1]\n",
    "    )\n",
    "    \n",
    "    test_auc = roc_auc_score(\n",
    "        y_test, model.predict_proba(X_test)[:, 1]\n",
    "    )\n",
    "    \n",
    "    print(f\"k={k} | Train AUC={train_auc:.3f} | Test AUC={test_auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbd51e1-2363-4d06-94af-ee1b083da7d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“Œ kNN & Distance Bias â€” Cheat Sheet (Interview-Ready)\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ What is kNN?\n",
    "\n",
    "* Distance-based supervised algorithm\n",
    "* No training phase (lazy learner)\n",
    "* Prediction = nearest `k` neighborsâ€™ vote/average\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ Distance Bias\n",
    "\n",
    "**Definition:**\n",
    "When features with larger numeric scales dominate distance calculation.\n",
    "\n",
    "**Key Rule:**\n",
    "\n",
    "> **kNN without feature scaling is mathematically wrong.**\n",
    "\n",
    "* Scaling required: `StandardScaler` / `MinMaxScaler`\n",
    "* Trees donâ€™t need scaling; kNN does\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Curse of Dimensionality\n",
    "\n",
    "* As feature count â†‘, all points become similarly distant\n",
    "* Nearest neighbor â‰ˆ farthest neighbor\n",
    "* Distance loses meaning â†’ kNN collapses\n",
    "\n",
    "**One-liner:**\n",
    "\n",
    "> â€œHigh dimensionality causes distance concentration.â€\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£ Biasâ€“Variance Control in kNN (via `k`)\n",
    "\n",
    "| k value       | Behavior    | Risk                            |\n",
    "| ------------- | ----------- | ------------------------------- |\n",
    "| Small k (1,3) | Very local  | **High variance (overfitting)** |\n",
    "| Medium k      | Balanced    | Best tradeoff                   |\n",
    "| Large k       | Over-smooth | **High bias (underfitting)**    |\n",
    "\n",
    "---\n",
    "\n",
    "## 5ï¸âƒ£ Why **k = 1** is Dangerous (IMPORTANT)\n",
    "\n",
    "Even if:\n",
    "\n",
    "```\n",
    "Train AUC = 1.0\n",
    "Test  AUC = 1.0\n",
    "```\n",
    "\n",
    "### âŒ Still NOT balanced\n",
    "\n",
    "**Why?**\n",
    "\n",
    "* Uses only **one data point**\n",
    "* Extremely sensitive to noise\n",
    "* **Stability is very low**\n",
    "* A single point moving/changing can flip predictions\n",
    "\n",
    "ðŸ‘‰ High test score here is due to **small dataset luck**, not true generalization.\n",
    "\n",
    "**Interview-ready line:**\n",
    "\n",
    "> â€œk=1 has extremely low stability; a small perturbation or noisy point can completely change predictions, indicating very high variance.â€\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£ How to Identify â€œBest Balancedâ€ Model\n",
    "\n",
    "âŒ Wrong logic:\n",
    "\n",
    "> Highest test AUC\n",
    "\n",
    "âœ… Correct logic:\n",
    "\n",
    "* Train score not perfect\n",
    "* Trainâ€“test gap small\n",
    "* Model behavior stable\n",
    "* Less sensitive to small data changes\n",
    "\n",
    "ðŸ‘‰ For small datasets: **prefer k = 3 or 5 over k = 1**\n",
    "\n",
    "---\n",
    "\n",
    "## 7ï¸âƒ£ Why Step-6 (varying k) is Mandatory in kNN\n",
    "\n",
    "* kNN has **no regularization**\n",
    "* No coefficients\n",
    "* `k` is the **only complexity control knob**\n",
    "\n",
    "ðŸ‘‰ So:\n",
    "\n",
    "> **Tuning and analyzing k is mandatory for every kNN model**\n",
    "\n",
    "Yes â€” **this applies to all kNN use cases**.\n",
    "\n",
    "---\n",
    "\n",
    "## 8ï¸âƒ£ Final One-Line Summary\n",
    "\n",
    "> â€œkNN performance depends entirely on distance and k; small k leads to high variance and instability, while larger k increases bias. Metrics alone can be misleading on small datasets.â€\n",
    "\n",
    "---\n",
    "\n",
    "## 9ï¸âƒ£ Ultra-Short Memory Hooks\n",
    "\n",
    "* kNN â†’ distance-based â†’ scaling mandatory\n",
    "* Small k â†’ memorization â†’ high variance\n",
    "* Large k â†’ over-smoothing â†’ high bias\n",
    "* k = 1 + perfect score â‰  good model\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f0dbac-339f-47ee-b7f1-330c7e224a56",
   "metadata": {},
   "source": [
    "Got it. Below is a **clear, structured:\n",
    "\n",
    "* **kNN advantages**\n",
    "* **kNN disadvantages & limitations**\n",
    "* **Why kNN is fundamentally different from Linear & Logistic Regression**\n",
    "* **Direct comparisons you can say in interviews**\n",
    "\n",
    "No fluff. This is the version you revise before interviews.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ“Œ kNN (k-Nearest Neighbors): Advantages, Disadvantages & Comparison\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ What makes kNN different (Big Picture)\n",
    "\n",
    "### kNN\n",
    "\n",
    "* **Instance-based / distance-based algorithm**\n",
    "* No explicit training phase\n",
    "* Stores data and decides **at prediction time**\n",
    "\n",
    "### Linear / Logistic Regression\n",
    "\n",
    "* **Parametric models**\n",
    "* Learn coefficients during training\n",
    "* Use a fixed mathematical form\n",
    "\n",
    "ðŸ‘‰ This single difference explains almost all behavior differences.\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ Advantages of kNN\n",
    "\n",
    "### âœ… Advantage 1: Simple and Intuitive\n",
    "\n",
    "* Easy to understand and explain\n",
    "* â€œSimilar points should have similar labelsâ€\n",
    "\n",
    "Good for:\n",
    "\n",
    "* Learning ML intuition\n",
    "* Quick baselines\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Advantage 2: No Training Phase\n",
    "\n",
    "* Training time â‰ˆ zero\n",
    "* Just stores data\n",
    "\n",
    "Good for:\n",
    "\n",
    "* Small datasets\n",
    "* Rapid experimentation\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Advantage 3: Can Model Non-Linear Patterns\n",
    "\n",
    "* Decision boundary can be very flexible\n",
    "* No assumption of linearity\n",
    "\n",
    "Why this matters:\n",
    "\n",
    "* Logistic Regression has a **linear decision boundary**\n",
    "* kNN can follow complex shapes\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Advantage 4: Assumption-Free\n",
    "\n",
    "* No assumption about data distribution\n",
    "* No assumption of linear relationship\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Disadvantages of kNN (Critical)\n",
    "\n",
    "### âŒ Disadvantage 1: Feature Scaling is Mandatory\n",
    "\n",
    "* kNN uses distance\n",
    "* Large-scale features dominate distance\n",
    "\n",
    "Without scaling:\n",
    "\n",
    "* Distance becomes meaningless\n",
    "* Predictions are wrong\n",
    "\n",
    "ðŸ‘‰ In Logistic Regression: scaling is important\n",
    "ðŸ‘‰ In kNN: scaling is **mandatory**\n",
    "\n",
    "---\n",
    "\n",
    "### âŒ Disadvantage 2: Curse of Dimensionality\n",
    "\n",
    "* As feature count increases:\n",
    "\n",
    "  * All points become similarly distant\n",
    "  * Nearest neighbor â‰ˆ farthest neighbor\n",
    "\n",
    "Result:\n",
    "\n",
    "* kNN loses discriminative power\n",
    "* Performance collapses\n",
    "\n",
    "---\n",
    "\n",
    "### âŒ Disadvantage 3: Slow Prediction\n",
    "\n",
    "* Prediction requires computing distance to **all training points**\n",
    "* Very slow for large datasets\n",
    "\n",
    "ðŸ‘‰ Logistic Regression prediction is fast.\n",
    "\n",
    "---\n",
    "\n",
    "### âŒ Disadvantage 4: High Memory Usage\n",
    "\n",
    "* Entire training dataset must be stored\n",
    "* Model size = data size\n",
    "\n",
    "---\n",
    "\n",
    "### âŒ Disadvantage 5: Noise Sensitivity\n",
    "\n",
    "* Especially for small k (e.g., k=1)\n",
    "* One noisy point can flip predictions\n",
    "\n",
    "ðŸ‘‰ High variance problem\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£ Limitations of kNN\n",
    "\n",
    "kNN performs poorly when:\n",
    "\n",
    "* Dataset is large\n",
    "* Feature space is high-dimensional\n",
    "* Data is noisy\n",
    "* Interpretability is required\n",
    "* Low-latency prediction is needed\n",
    "\n",
    "---\n",
    "\n",
    "## 5ï¸âƒ£ kNN vs Linear / Logistic Regression (Direct Comparison)\n",
    "\n",
    "| Aspect                  | kNN            | Linear / Logistic Regression |\n",
    "| ----------------------- | -------------- | ---------------------------- |\n",
    "| Model type              | Instance-based | Parametric                   |\n",
    "| Training                | No training    | Learns coefficients          |\n",
    "| Prediction speed        | Slow           | Fast                         |\n",
    "| Feature scaling         | Mandatory      | Important, not mandatory     |\n",
    "| Handles high dimensions | Poorly         | Better                       |\n",
    "| Interpretability        | None           | High                         |\n",
    "| Biasâ€“variance control   | Via k          | Via regularization           |\n",
    "| Production usage        | Rare           | Very common                  |\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£ Biasâ€“Variance Comparison\n",
    "\n",
    "### kNN\n",
    "\n",
    "* Small k â†’ **High variance** (overfitting)\n",
    "* Large k â†’ **High bias** (underfitting)\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "* High bias by design (linear boundary)\n",
    "* Variance controlled using **L1 / L2 regularization**\n",
    "* Much more stable\n",
    "\n",
    "---\n",
    "\n",
    "## 7ï¸âƒ£ Why kNN behaves differently from Logistic Regression\n",
    "\n",
    "### kNN\n",
    "\n",
    "* Local, distance-based decisions\n",
    "* Highly sensitive to data distribution\n",
    "* Behavior changes with small data perturbations\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "* Global decision boundary\n",
    "* Learns once, predicts consistently\n",
    "* More robust and stable\n",
    "\n",
    "ðŸ‘‰ This is why Logistic Regression is preferred in production.\n",
    "\n",
    "---\n",
    "\n",
    "## 8ï¸âƒ£ When to Use kNN\n",
    "\n",
    "Use kNN when:\n",
    "\n",
    "* Dataset is small\n",
    "* Feature count is low\n",
    "* Features are numeric\n",
    "* You want a quick non-linear baseline\n",
    "* You are doing exploratory analysis\n",
    "\n",
    "---\n",
    "\n",
    "## 9ï¸âƒ£ When NOT to Use kNN\n",
    "\n",
    "Avoid kNN when:\n",
    "\n",
    "* Dataset is large\n",
    "* Feature count is high\n",
    "* Interpretability matters\n",
    "* Prediction speed matters\n",
    "* Model stability is critical\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”‘ Interview-Ready One-Liners (Very Important)\n",
    "\n",
    "* â€œkNN is distance-based, so feature scaling is mandatory.â€\n",
    "* â€œkNN suffers from the curse of dimensionality.â€\n",
    "* â€œkNN has no training phase but expensive inference.â€\n",
    "* â€œLogistic Regression is more stable due to regularization.â€\n",
    "* â€œkNN memorizes; Logistic Regression generalizes.â€\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”Ÿ Final Summary (Perfect Interview Answer)\n",
    "\n",
    "> kNN is a simple, distance-based algorithm that can model non-linear patterns without assumptions, but it suffers from high variance, poor scalability, sensitivity to feature scaling, and the curse of dimensionality. In contrast, Linear and Logistic Regression are parametric, stable, interpretable models that generalize better and are more suitable for production systems.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fbdb69-ed94-4426-8817-28320e0ffaa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec463fd-8595-43ac-9be3-a2d6bd1d4b31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
