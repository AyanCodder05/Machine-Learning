{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82868bc9-d1a2-45ab-84c3-77042ac769eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_dataset.csv created with shape: (20000, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "rows = 20000\n",
    "\n",
    "age = np.random.randint(21, 60, rows)\n",
    "income = np.random.randint(200000, 1500000, rows)\n",
    "loan_amount = np.random.randint(50000, 800000, rows)\n",
    "\n",
    "employment_type = np.random.choice(\n",
    "    ['Salaried', 'Self-Employed', 'Unemployed'],\n",
    "    rows,\n",
    "    p=[0.6, 0.3, 0.1]\n",
    ")\n",
    "\n",
    "education = np.random.choice(\n",
    "    ['Graduate', 'Post-Graduate', 'High School'],\n",
    "    rows,\n",
    "    p=[0.5, 0.3, 0.2]\n",
    ")\n",
    "\n",
    "marital_status = np.random.choice(\n",
    "    ['Single', 'Married', 'Divorced'],\n",
    "    rows,\n",
    "    p=[0.45, 0.45, 0.10]\n",
    ")\n",
    "\n",
    "loan_purpose = np.random.choice(\n",
    "    ['Home', 'Car', 'Education', 'Business', 'Personal'],\n",
    "    rows\n",
    ")\n",
    "\n",
    "# Target logic (realistic, noisy)\n",
    "default = (\n",
    "    (income < 400000) &\n",
    "    (loan_amount > income * 0.6) &\n",
    "    (employment_type == 'Unemployed')\n",
    ").astype(int)\n",
    "\n",
    "# Add noise so dataset is not perfect\n",
    "noise_idx = np.random.choice(rows, int(0.1 * rows), replace=False)\n",
    "default[noise_idx] = 1 - default[noise_idx]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'age': age,\n",
    "    'income': income,\n",
    "    'loan_amount': loan_amount,\n",
    "    'employment_type': employment_type,\n",
    "    'education': education,\n",
    "    'marital_status': marital_status,\n",
    "    'loan_purpose': loan_purpose,\n",
    "    'default': default\n",
    "})\n",
    "\n",
    "df.to_csv(\"loan_dataset.csv\", index=False)\n",
    "print(\"loan_dataset.csv created with shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96953cd1-5d9d-4ffd-b3c8-58d0acba9487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>loan_amount</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>education</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59</td>\n",
       "      <td>604067</td>\n",
       "      <td>585004</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Married</td>\n",
       "      <td>Home</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>1422329</td>\n",
       "      <td>110644</td>\n",
       "      <td>Self-Employed</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Single</td>\n",
       "      <td>Home</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>969855</td>\n",
       "      <td>699938</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>Post-Graduate</td>\n",
       "      <td>Married</td>\n",
       "      <td>Education</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>1253869</td>\n",
       "      <td>792784</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>High School</td>\n",
       "      <td>Married</td>\n",
       "      <td>Car</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>705365</td>\n",
       "      <td>775076</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Single</td>\n",
       "      <td>Home</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age   income  loan_amount employment_type      education marital_status  \\\n",
       "0   59   604067       585004        Salaried       Graduate        Married   \n",
       "1   49  1422329       110644   Self-Employed       Graduate         Single   \n",
       "2   35   969855       699938        Salaried  Post-Graduate        Married   \n",
       "3   28  1253869       792784        Salaried    High School        Married   \n",
       "4   41   705365       775076        Salaried       Graduate         Single   \n",
       "\n",
       "  loan_purpose  default  \n",
       "0         Home        0  \n",
       "1         Home        0  \n",
       "2    Education        0  \n",
       "3          Car        0  \n",
       "4         Home        0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"loan_dataset.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "481c38cf-c88a-440a-a8a7-c763e39b8d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('default',axis=1)\n",
    "y=df['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "384d8f7a-d205-488b-bfcc-ad014aab3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cccb5e02-c796-407a-99f1-c5e8acf2c660",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'l2', 'elasticnet'} or None. Got 'none' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 34\u001b[0m\n\u001b[0;32m     19\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m ColumnTransformer([\n\u001b[0;32m     20\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m\"\u001b[39m, numeric_pipeline, num_cols),\n\u001b[0;32m     21\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m\"\u001b[39m, categorical_pipeline, cat_cols)\n\u001b[0;32m     22\u001b[0m ])\n\u001b[0;32m     25\u001b[0m model_no_reg \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m     26\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessing\u001b[39m\u001b[38;5;124m\"\u001b[39m, preprocessor),\n\u001b[0;32m     27\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, LogisticRegression(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     ))\n\u001b[0;32m     32\u001b[0m ])\n\u001b[1;32m---> 34\u001b[0m model_no_reg\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     36\u001b[0m train_auc \u001b[38;5;241m=\u001b[39m roc_auc_score(y_train, model_no_reg\u001b[38;5;241m.\u001b[39mpredict_proba(X_train)[:,\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     37\u001b[0m test_auc \u001b[38;5;241m=\u001b[39m roc_auc_score(y_test, model_no_reg\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:,\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:662\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    657\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    658\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    659\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    660\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    661\u001b[0m         )\n\u001b[1;32m--> 662\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1382\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1377\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1378\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1379\u001b[0m )\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[0;32m   1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:436\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    429\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 436\u001b[0m     validate_parameter_constraints(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameter_constraints,\n\u001b[0;32m    438\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m    439\u001b[0m         caller_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    440\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:98\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     96\u001b[0m     )\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'l2', 'elasticnet'} or None. Got 'none' instead."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ‡¶ï‡ßá‡¶® stratify?\n",
    "\n",
    "# Default usually imbalanced\n",
    "\n",
    "# Class ratio preserve ‡¶ï‡¶∞‡¶§‡ßá\n",
    "\n",
    "num_cols= [\"age\", \"income\", \"loan_amount\"]\n",
    "cat_cols=X_train.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    (\"onehot\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_pipeline, num_cols),\n",
    "    (\"cat\", categorical_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "\n",
    "model_no_reg = Pipeline([\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"model\", LogisticRegression(\n",
    "        penalty=\"none\",\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000\n",
    "    ))\n",
    "])\n",
    "\n",
    "model_no_reg.fit(X_train, y_train)\n",
    "\n",
    "train_auc = roc_auc_score(y_train, model_no_reg.predict_proba(X_train)[:,1])\n",
    "test_auc = roc_auc_score(y_test, model_no_reg.predict_proba(X_test)[:,1])\n",
    "\n",
    "print(train_auc, test_auc)\n",
    "\n",
    "model_l2 = Pipeline([\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"model\", LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        C=0.01,\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000\n",
    "    ))\n",
    "])\n",
    "\n",
    "model_l2.fit(X_train, y_train)\n",
    "\n",
    "train_auc_l2 = roc_auc_score(y_train, model_l2.predict_proba(X_train)[:,1])\n",
    "test_auc_l2 = roc_auc_score(y_test, model_l2.predict_proba(X_test)[:,1])\n",
    "\n",
    "print(train_auc_l2, test_auc_l2)\n",
    "\n",
    "model_l1 = Pipeline([\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"model\", LogisticRegression(\n",
    "        penalty=\"l1\",\n",
    "        C=0.1,\n",
    "        solver=\"liblinear\",\n",
    "        max_iter=1000\n",
    "    ))\n",
    "])\n",
    "\n",
    "model_l1.fit(X_train, y_train)\n",
    "\n",
    "train_auc_l1 = roc_auc_score(y_train, model_l1.predict_proba(X_train)[:,1])\n",
    "test_auc_l1 = roc_auc_score(y_test, model_l1.predict_proba(X_test)[:,1])\n",
    "\n",
    "print(train_auc_l1, test_auc_l1)\n",
    "\n",
    "feature_names = model_l1.named_steps[\"preprocessing\"].get_feature_names_out()\n",
    "coeffs = model_l1.named_steps[\"model\"].coef_[0]\n",
    "\n",
    "non_zero = np.sum(coeffs != 0)\n",
    "print(\"Non-zero features:\", non_zero)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    model_l2,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\"\n",
    ")\n",
    "\n",
    "print(\"CV Mean:\", cv_scores.mean())\n",
    "print(\"CV Std :\", cv_scores.std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4528f7b-c3af-499d-abb0-ab8a1adb8160",
   "metadata": {},
   "source": [
    "‡¶≠‡¶æ‡¶≤‡ßã‡•§ ‡¶§‡ßÅ‡¶Æ‡¶ø **Phase-3 Day-1 code ‡¶†‡¶ø‡¶ï‡¶≠‡¶æ‡¶¨‡ßá‡¶á implement ‡¶ï‡¶∞‡ßá‡¶õ‡ßã**‡•§\n",
    "‡¶è‡¶ñ‡¶® ‡¶Ü‡¶Æ‡¶ø **no-nonsense evaluator mode-‡¶è** ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡¶ø‡•§\n",
    "\n",
    "‡¶Ü‡¶Æ‡¶ø ‡ß©‡¶ü‡¶æ ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶¨:\n",
    "\n",
    "1. üîç **Code review (engineering-level)**\n",
    "2. üß† **Result ‡¶ï‡ßÄ ‡¶Ü‡¶∏‡¶æ‡¶∞ ‡¶ï‡¶•‡¶æ + ‡¶ï‡ßÄ‡¶≠‡¶æ‡¶¨‡ßá interpret ‡¶ï‡¶∞‡¶¨‡ßá**\n",
    "3. ‚ùå **Common thinking mistakes + corrections**\n",
    "4. ‚úÖ **Final verdict: pass / partial / fix-needed**\n",
    "\n",
    "> ‚ö†Ô∏è ‡¶§‡ßÅ‡¶Æ‡¶ø output paste ‡¶ï‡¶∞‡ßã‡¶®‡¶ø, ‡¶§‡¶æ‡¶á ‡¶Ü‡¶Æ‡¶ø **expected behavior + diagnosis logic** ‡¶¶‡¶ø‡¶ö‡ßç‡¶õ‡¶ø‡•§\n",
    "> Output ‡¶¶‡¶ø‡¶≤‡ßá ‡¶Ü‡¶Æ‡¶ø exact judgement ‡¶¶‡ßá‡¶¨‡•§\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Code Review ‚Äî Strict\n",
    "\n",
    "### ‚úÖ What you did RIGHT (important)\n",
    "\n",
    "* `train_test_split` **before preprocessing** ‚Üí no leakage ‚úîÔ∏è\n",
    "* `stratify=y` ‚Üí imbalanced target handled ‚úîÔ∏è\n",
    "* `Pipeline + ColumnTransformer` ‚Üí production-safe ‚úîÔ∏è\n",
    "* Same preprocessing for all models ‚Üí fair comparison ‚úîÔ∏è\n",
    "* Separate **no-reg / L2 / L1** ‚Üí correct experiment design ‚úîÔ∏è\n",
    "* ROC-AUC used (not accuracy) ‚Üí correct metric ‚úîÔ∏è\n",
    "* CV done on **pipeline**, not raw data ‚Üí big plus ‚úîÔ∏è\n",
    "\n",
    "üëâ From an interview perspective:\n",
    "**This code alone already puts you above average candidates.**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Minor technical issue (but I must call it out)\n",
    "\n",
    "You used:\n",
    "\n",
    "```python\n",
    "non_zero = np.sum(coeffs != 0)\n",
    "```\n",
    "\n",
    "But I don‚Äôt see:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "If this ran, it means you imported it earlier.\n",
    "If not ‚Üí **NameError**.\n",
    "\n",
    "üëâ In interview or production, this kind of miss is a red flag.\n",
    "Be clean.\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Expected Results & Interpretation (THIS IS THE REAL TEST)\n",
    "\n",
    "### üîπ Model A ‚Äî No Regularization\n",
    "\n",
    "**Expected pattern:**\n",
    "\n",
    "```text\n",
    "Train AUC: high (‚âà 0.90+)\n",
    "Test AUC : noticeably lower\n",
    "```\n",
    "\n",
    "**Interpretation (you must say this):**\n",
    "\n",
    "> ‚ÄúNo regularization + high-dimensional data caused the model to fit noise, resulting in high variance.‚Äù\n",
    "\n",
    "If your **train ‚â´ test**, you MUST say **high variance**, not ‚Äúmodel is good‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Model B ‚Äî L2 Regularization (`C=0.01`)\n",
    "\n",
    "**Expected pattern:**\n",
    "\n",
    "```text\n",
    "Train AUC: lower than no-reg\n",
    "Test AUC : higher or more stable\n",
    "CV Std   : low\n",
    "```\n",
    "\n",
    "**Correct reasoning:**\n",
    "\n",
    "> ‚ÄúL2 shrinks coefficients uniformly, reducing variance while keeping correlated features, leading to a more stable model.‚Äù\n",
    "\n",
    "If test AUC improves or CV std drops ‚Üí **L2 is working**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Model C ‚Äî L1 Regularization\n",
    "\n",
    "**Expected pattern:**\n",
    "\n",
    "```text\n",
    "Train AUC: may drop\n",
    "Test AUC : may increase or fluctuate\n",
    "Non-zero features: significantly fewer\n",
    "```\n",
    "\n",
    "**Key insight (THIS is interview gold):**\n",
    "\n",
    "> ‚ÄúL1 performs feature selection, but in presence of correlated features, the selected set may be unstable across folds.‚Äù\n",
    "\n",
    "If non-zero features are **very few**, that‚Äôs expected.\n",
    "But stability matters more than count.\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Cross-Validation Result ‚Äî How to Read It\n",
    "\n",
    "```python\n",
    "CV Mean: X\n",
    "CV Std : Y\n",
    "```\n",
    "\n",
    "### Correct logic:\n",
    "\n",
    "* **High CV mean + low CV std** ‚Üí deploy-safe\n",
    "* **High CV mean + high CV std** ‚Üí risky\n",
    "* **Slightly lower mean + very low std** ‚Üí often better\n",
    "\n",
    "üëâ If you can explain **why std matters**, you pass interviews.\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ The 3 Questions I Asked ‚Äî Model Answers\n",
    "\n",
    "### Q1Ô∏è‚É£ Which model overfit the most? Why?\n",
    "\n",
    "**Correct answer:**\n",
    "\n",
    "> The no-regularization model overfit the most because high-dimensional OneHotEncoded features allowed it to learn noise, resulting in high train AUC but poor generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### Q2Ô∏è‚É£ Why was L2 more stable than L1?\n",
    "\n",
    "**Correct answer:**\n",
    "\n",
    "> L2 keeps all correlated features with reduced magnitude, whereas L1 arbitrarily drops correlated features, making it unstable across different data splits.\n",
    "\n",
    "---\n",
    "\n",
    "### Q3Ô∏è‚É£ If business wants interpretability, which model?\n",
    "\n",
    "**Correct answer (strict):**\n",
    "\n",
    "> Logistic Regression with L2 regularization, because it preserves all features with stable coefficients while still controlling variance.\n",
    "\n",
    "‚ùå Saying ‚ÄúL1 because feature selection‚Äù **without mentioning instability** is WRONG.\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Common Mistakes (You did NOT make them ‚Äî good)\n",
    "\n",
    "* ‚ùå Using accuracy instead of AUC\n",
    "* ‚ùå Doing CV after preprocessing\n",
    "* ‚ùå Comparing models with different pipelines\n",
    "* ‚ùå Blind GridSearch\n",
    "\n",
    "You avoided all of these.\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Final Verdict (Honest)\n",
    "\n",
    "### ‚úÖ Phase 3 ‚Äì Day 1: **PASSED (with minor polish needed)**\n",
    "\n",
    "What you still need:\n",
    "\n",
    "* Faster verbal explanation\n",
    "* Confidence in bias‚Äìvariance language\n",
    "* Practice defending decisions under pressure\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ed6fe4-58ec-496c-a8f8-e2dcdefb3560",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Why **stratify** (even though not shown explicitly here)\n",
    "\n",
    "You wrote:\n",
    "\n",
    "> Default usually imbalanced\n",
    "> Class ratio preserve ‡¶ï‡¶∞‡¶§‡ßá\n",
    "\n",
    "That‚Äôs **correct**, but incomplete.\n",
    "\n",
    "### What stratify actually does\n",
    "\n",
    "When you do:\n",
    "\n",
    "```python\n",
    "train_test_split(X, y, stratify=y)\n",
    "```\n",
    "\n",
    "It ensures:\n",
    "\n",
    "* **Train and test both have the same class proportion**\n",
    "* Especially critical when:\n",
    "\n",
    "  * Target = `default`, `fraud`, `churn`, `disease`\n",
    "  * Minority class is small (5‚Äì20%)\n",
    "\n",
    "### Brutal truth (interview trap)\n",
    "\n",
    "If you **don‚Äôt stratify** and still report:\n",
    "\n",
    "* ROC-AUC\n",
    "* F1\n",
    "* Recall\n",
    "\n",
    "üëâ **Your evaluation is unreliable**\n",
    "üëâ Interviewer may say: *‚ÄúYour test set doesn‚Äôt represent real distribution‚Äù*\n",
    "\n",
    "**One-line interview answer**\n",
    "\n",
    "> ‚ÄúI used stratified split to preserve the class distribution so that evaluation metrics like ROC-AUC are not biased by sampling artifacts.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Feature grouping\n",
    "\n",
    "```python\n",
    "num_cols = [\"age\", \"income\", \"loan_amount\"]\n",
    "cat_cols = X_train.select_dtypes(include=\"object\").columns.tolist()\n",
    "```\n",
    "\n",
    "### What this does\n",
    "\n",
    "* Explicitly separates:\n",
    "\n",
    "  * **Numerical features** ‚Üí scaling\n",
    "  * **Categorical features** ‚Üí one-hot encoding\n",
    "\n",
    "### Why it matters\n",
    "\n",
    "Logistic Regression assumes:\n",
    "\n",
    "* Features are **numeric**\n",
    "* Coefficients are **comparable in scale**\n",
    "\n",
    "### Interview angle\n",
    "\n",
    "If you don‚Äôt separate numeric & categorical:\n",
    "\n",
    "* Scaling breaks categories\n",
    "* Model assumptions violated\n",
    "\n",
    "**Interview line**\n",
    "\n",
    "> ‚ÄúI explicitly separated numerical and categorical features because they require different preprocessing transformations.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Numeric pipeline\n",
    "\n",
    "```python\n",
    "numeric_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "```\n",
    "\n",
    "### What it does\n",
    "\n",
    "* Converts features to:\n",
    "\n",
    "  * Mean = 0\n",
    "  * Std = 1\n",
    "\n",
    "### Why it‚Äôs REQUIRED for Logistic Regression\n",
    "\n",
    "Logistic Regression uses:\n",
    "\n",
    "* Gradient-based optimization\n",
    "* Regularization (L1 / L2)\n",
    "\n",
    "Without scaling:\n",
    "\n",
    "* Large-value features dominate gradients\n",
    "* Regularization becomes meaningless\n",
    "\n",
    "### Brutal truth\n",
    "\n",
    "If someone says:\n",
    "\n",
    "> ‚ÄúScaling doesn‚Äôt matter for LR‚Äù\n",
    "\n",
    "They don‚Äôt understand optimization.\n",
    "\n",
    "**Interview one-liner**\n",
    "\n",
    "> ‚ÄúStandardScaler is required because Logistic Regression is scale-sensitive and regularization assumes comparable feature magnitudes.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Categorical pipeline\n",
    "\n",
    "```python\n",
    "categorical_pipeline = Pipeline([\n",
    "    (\"onehot\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"))\n",
    "])\n",
    "```\n",
    "\n",
    "### What this does\n",
    "\n",
    "* Converts categories ‚Üí binary vectors\n",
    "* `drop=\"first\"` avoids:\n",
    "\n",
    "  * Dummy variable trap\n",
    "  * Multicollinearity\n",
    "\n",
    "### `handle_unknown=\"ignore\"` ‚Äî very important\n",
    "\n",
    "* Prevents crash when:\n",
    "\n",
    "  * Test data has unseen category\n",
    "* Without it ‚Üí **production failure**\n",
    "\n",
    "### Interview trap\n",
    "\n",
    "If interviewer asks:\n",
    "\n",
    "> ‚ÄúWhy drop first category?‚Äù\n",
    "\n",
    "Correct answer:\n",
    "\n",
    "> ‚ÄúTo avoid perfect multicollinearity which destabilizes coefficient estimation in linear models.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ ColumnTransformer\n",
    "\n",
    "```python\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_pipeline, num_cols),\n",
    "    (\"cat\", categorical_pipeline, cat_cols)\n",
    "])\n",
    "```\n",
    "\n",
    "### What it does\n",
    "\n",
    "* Applies:\n",
    "\n",
    "  * Scaling ‚Üí numeric columns\n",
    "  * One-hot ‚Üí categorical columns\n",
    "* Keeps everything aligned\n",
    "\n",
    "### Why this is professional-grade\n",
    "\n",
    "* Prevents **data leakage**\n",
    "* Ensures same preprocessing in:\n",
    "\n",
    "  * Train\n",
    "  * Test\n",
    "  * Cross-validation\n",
    "  * Production\n",
    "\n",
    "**Interview gold**\n",
    "\n",
    "> ‚ÄúI used ColumnTransformer to ensure feature-wise preprocessing without leakage.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Logistic Regression without regularization\n",
    "\n",
    "```python\n",
    "model_no_reg = Pipeline([\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"model\", LogisticRegression(\n",
    "        penalty=\"none\",\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000\n",
    "    ))\n",
    "])\n",
    "```\n",
    "\n",
    "### What this model is\n",
    "\n",
    "* Pure Maximum Likelihood Estimation\n",
    "* No penalty on coefficients\n",
    "\n",
    "### Why you train this\n",
    "\n",
    "* **Baseline**\n",
    "* To detect:\n",
    "\n",
    "  * Overfitting\n",
    "  * Coefficient explosion\n",
    "\n",
    "### Brutal reality\n",
    "\n",
    "If:\n",
    "\n",
    "```text\n",
    "train_auc >> test_auc\n",
    "```\n",
    "\n",
    "üëâ You‚Äôre overfitting\n",
    "\n",
    "### Interview explanation\n",
    "\n",
    "> ‚ÄúI first trained an unregularized model to establish a baseline and observe overfitting behavior.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ ROC-AUC calculation\n",
    "\n",
    "```python\n",
    "roc_auc_score(y_train, model.predict_proba(X_train)[:,1])\n",
    "```\n",
    "\n",
    "### Why ROC-AUC\n",
    "\n",
    "* Threshold-independent\n",
    "* Robust to imbalance\n",
    "* Measures ranking quality\n",
    "\n",
    "### Interview trap\n",
    "\n",
    "If someone uses `.predict()` instead of `.predict_proba()`:\n",
    "üëâ **Wrong for AUC**\n",
    "\n",
    "Correct explanation:\n",
    "\n",
    "> ‚ÄúROC-AUC evaluates how well the model ranks positives above negatives across all thresholds.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£ L2 Regularization (Ridge)\n",
    "\n",
    "```python\n",
    "LogisticRegression(\n",
    "    penalty=\"l2\",\n",
    "    C=0.01\n",
    ")\n",
    "```\n",
    "\n",
    "### What L2 does\n",
    "\n",
    "* Shrinks coefficients\n",
    "* Keeps all features\n",
    "* Reduces variance\n",
    "\n",
    "### Meaning of `C`\n",
    "\n",
    "* **Inverse** of regularization strength\n",
    "* Smaller C ‚Üí stronger penalty\n",
    "\n",
    "### Why L2 is default in industry\n",
    "\n",
    "* Stable\n",
    "* Differentiable\n",
    "* Works well with correlated features\n",
    "\n",
    "**Interview line**\n",
    "\n",
    "> ‚ÄúL2 regularization controls overfitting by shrinking coefficients without removing features.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## 9Ô∏è‚É£ L1 Regularization (Lasso)\n",
    "\n",
    "```python\n",
    "penalty=\"l1\",\n",
    "solver=\"liblinear\"\n",
    "```\n",
    "\n",
    "### What L1 does\n",
    "\n",
    "* Forces coefficients ‚Üí exactly zero\n",
    "* Performs **embedded feature selection**\n",
    "\n",
    "### Why solver changes\n",
    "\n",
    "* `lbfgs` ‚ùå doesn‚Äôt support L1\n",
    "* `liblinear` ‚úÖ supports L1\n",
    "\n",
    "### Interview bomb answer\n",
    "\n",
    "> ‚ÄúL1 regularization performs feature selection by driving weak feature coefficients to zero.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## üîü Counting non-zero features\n",
    "\n",
    "```python\n",
    "coeffs = model_l1.named_steps[\"model\"].coef_[0]\n",
    "non_zero = np.sum(coeffs != 0)\n",
    "```\n",
    "\n",
    "### What this proves\n",
    "\n",
    "* How many features survived L1 penalty\n",
    "* Model sparsity\n",
    "* Interpretability\n",
    "\n",
    "### Interview usage\n",
    "\n",
    "> ‚ÄúI used L1 to reduce dimensionality and checked sparsity by counting non-zero coefficients.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£1Ô∏è‚É£ Cross-validation\n",
    "\n",
    "```python\n",
    "cross_val_score(\n",
    "    model_l2,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Why CV\n",
    "\n",
    "* Single split is unstable\n",
    "* CV estimates **generalization performance**\n",
    "\n",
    "### Why CV only on training set\n",
    "\n",
    "üëâ If you CV on full data ‚Üí **test leakage**\n",
    "\n",
    "### Interview-perfect answer\n",
    "\n",
    "> ‚ÄúI used 5-fold cross-validation on the training set to estimate variance and stability of ROC-AUC.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è What‚Äôs missing / what interviewer may attack you on\n",
    "\n",
    "Be ready for these:\n",
    "\n",
    "1Ô∏è‚É£ **Where is stratify used?**\n",
    "‚Üí You must mention it during `train_test_split`\n",
    "\n",
    "2Ô∏è‚É£ **Why not class_weight='balanced'?**\n",
    "‚Üí If imbalance is severe, this matters\n",
    "\n",
    "3Ô∏è‚É£ **How did you choose C=0.01 / 0.1?**\n",
    "‚Üí Should say: *‚Äúvia validation or GridSearchCV‚Äù*\n",
    "\n",
    "4Ô∏è‚É£ **Why ROC-AUC over F1?**\n",
    "‚Üí Threshold independence\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Final interview-ready summary (memorize this)\n",
    "\n",
    "> ‚ÄúI built a Logistic Regression pipeline with proper preprocessing using ColumnTransformer to avoid leakage. I compared unregularized, L2, and L1 models using ROC-AUC due to class imbalance. L2 reduced overfitting, while L1 performed feature selection. I validated stability using 5-fold cross-validation.‚Äù\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86828d18-998b-4ad8-996e-04851960c8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
